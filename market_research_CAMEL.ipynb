{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqyEmScr41QRcd+MoJYk1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nullzero-live/productAgent/blob/main/market_research_CAMEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we'll walk you through the development of an end-to-end Machine Learning software project that leverages an exciting and cutting-edge concept: CAMEL LangChain. We'll guide you from product description to deployment on Google Cloud Platform, using a wide range of software tools to ensure a comprehensive understanding.\n",
        "\n",
        "Phase 1: Experimenting with CAMEL LangChain\n",
        "This phase of the project has the lowest probability of success but is the most exciting conceptually. The idea published by the authors is philosophically and technologically interesting.\n",
        "\n",
        "CAMEL LangChain is an implementation found on Github that employs a unique approach called \"inception prompting.\" This approach encourages a pair of agents to be guided toward a goal. In this experimental phase, we'll treat these agents like dueling musicians, each with a common goal. The authors of the paper attempt to understand the \"cognition\" of the Large Language Models (LLMs) these agents are part of.\n",
        "\n",
        "To learn more about CAMEL LangChain, you can visit the project website or read the Arxiv paper.\n",
        "\n",
        "Steps in this phase\n",
        "Create a pair of agents discussing the startup's market: Our goal is to have these agents analyze not only the options of the market but also its depth, if possible in the implementation.\n",
        "\n",
        "Compress the discussion: Extract keywords from the agent's conversation and organize them into a data structure.\n",
        "\n",
        "Embed the discussion using OpenAI Ada embeddings: Transform the compressed output into a suitable format for storage.\n",
        "\n",
        "Finally, we'll store the compressed output in a vector store on Pinecone."
      ],
      "metadata": {
        "id": "eEJ50qELE0of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source https://python.langchain.com/en/latest/use_cases/agent_simulations/camel_role_playing.html"
      ],
      "metadata": {
        "id": "kStdLP1QD5B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' A pair of agents discussing the startup idea and storing the output\n",
        "in a vectorstore on pinecone '''\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from typing import List\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    BaseMessage,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ycHPfjhqDsIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVVB9ylODBdE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Setup a CAMEL agent helper class to manage \n",
        "\n",
        "class CAMELAgent:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        system_message: SystemMessage,\n",
        "        model: ChatOpenAI,\n",
        "    ) -> None:\n",
        "        self.system_message = system_message\n",
        "        self.model = model\n",
        "        self.init_messages()\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.init_messages()\n",
        "        return self.stored_messages\n",
        "\n",
        "    def init_messages(self) -> None:\n",
        "        self.stored_messages = [self.system_message]\n",
        "\n",
        "    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n",
        "        self.stored_messages.append(message)\n",
        "        return self.stored_messages\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        input_message: HumanMessage,\n",
        "    ) -> AIMessage:\n",
        "        messages = self.update_messages(input_message)\n",
        "\n",
        "        output_message = self.model(messages)\n",
        "        self.update_messages(output_message)\n",
        "\n",
        "        return output_message"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up roles for role playing\n",
        "\n",
        "assistant_role_name = \"Market Researcher\"\n",
        "user_role_name = \"UX Researcher\"\n",
        "task = \"Given an idea for a new technology business, perform market research which includes: gauging market size, how to describe the target customer, what methods of research are suitable.\"\n",
        "word_limit = 25 # word limit for task brainstorming"
      ],
      "metadata": {
        "id": "6wo77ZfoE7Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step it would be ideal if the System Message could be the output of a compressed LLM output to hone in on the goal more quickly.\n",
        "\n",
        "On further reading the CAMEL Framework is not able to be integrated with the other tools in the LangChain Library. The prompts are also carefully crafted, in order to maintain safety and cannot be changed."
      ],
      "metadata": {
        "id": "eqrzX_-RFoJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_specifier_sys_msg = SystemMessage(content=\"Restrict Research to USA only\")\n",
        "task_specifier_prompt = (\n",
        "\"\"\"Here is a task that {assistant_role_name} will help {user_role_name} to complete: {task}.\n",
        "Please make it more specific. Be creative and imaginative.\n",
        "Please reply with the specified task in {word_limit} words or less. Do not add anything else.\"\"\"\n",
        ")\n",
        "task_specifier_template = HumanMessagePromptTemplate.from_template(template=task_specifier_prompt)\n",
        "task_specify_agent = CAMELAgent(task_specifier_sys_msg, ChatOpenAI(temperature=1.1))\n",
        "task_specifier_msg = task_specifier_template.format_messages(assistant_role_name=assistant_role_name,\n",
        "                                                             user_role_name=user_role_name,\n",
        "                                                             task=task, word_limit=word_limit)[0]\n",
        "specified_task_msg = task_specify_agent.step(task_specifier_msg)\n",
        "print(f\"Specified task: {specified_task_msg.content}\")\n",
        "specified_task = specified_task_msg.content"
      ],
      "metadata": {
        "id": "jU7dkZt1Fff2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}